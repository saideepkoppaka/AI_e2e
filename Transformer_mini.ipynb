{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4f226e",
   "metadata": {},
   "source": [
    "# Transformer — detailed explanatory notes \n",
    "\n",
    "---\n",
    "\n",
    "## Why this mini-Transformer matters (Relevance)\n",
    "This notebook demonstrates the minimal building blocks of a Transformer-style decoder block. It is intended for teaching: showing shapes, tensor flows, and the actual numeric effect of attention. Real models are much larger and include LayerNorm, residual connections, dropout, tokenization subtleties, larger datasets, and training regimes — but the core ideas are captured here.\n",
    "\n",
    "Learning goals:\n",
    "- Understand how tokens are embedded and how positional information is added.\n",
    "- See how Q/K/V are produced and used to form attention scores.\n",
    "- Learn why we scale the dot-products and apply softmax.\n",
    "- Observe how masking enforces autoregressive behaviour.\n",
    "\n",
    "---\n",
    "\n",
    "## Transformer block: high-level structure\n",
    "A single decoder block (as implemented in this notebook) has:\n",
    "1. Input embeddings + positional embedding -> per-position vectors.\n",
    "2. Self-attention (multi-head):\n",
    "   - Linear projections produce Query (Q), Key (K), Value (V) matrices for each token.\n",
    "   - For each head, compute attention scores = Q · K^T, scale, mask, softmax -> attention weights.\n",
    "   - Use attention weights to compute a weighted sum of V (the attended representation).\n",
    "   - Concatenate heads and optionally project back to the model dimension.\n",
    "3. Feedforward (position-wise MLP):\n",
    "   - Applied independently at each position (same weights for all positions).\n",
    "4. (Not shown here) Typical production blocks add residual connections and LayerNorm between sublayers.\n",
    "\n",
    "Why this separation?\n",
    "- Self-attention aggregates information from other positions.\n",
    "- Feedforward does per-position non-linear transformation and feature mixing.\n",
    "\n",
    "---\n",
    "\n",
    "## Q, K, V — what they are and intuition\n",
    "- Key (K): a representation of what each source token \"offers\". Think of it as an index entry describing a token's content in the attention space.\n",
    "- Query (Q): a representation of what the target token is \"looking for\" — the current token's request vector.\n",
    "- Value (V): the payload — information that will be combined into the output if the key matches the query.\n",
    "\n",
    "Analogy: Suppose you're searching a library:\n",
    "- Keys = the catalog entries describing each book.\n",
    "- Values = the content inside each book.\n",
    "- Query = the search you type into the catalog.\n",
    "When a query matches a key, you retrieve that book's content (value).\n",
    "\n",
    "Mechanically:\n",
    "- Q and K live in the same vector space for similarity comparisons (dot product).\n",
    "- Dot(Q, K) produces a raw similarity score: how well does a particular source token (K) match the query?\n",
    "- Scores are turned into weights via softmax and used to compute a weighted sum of V.\n",
    "\n",
    "---\n",
    "\n",
    "## Scaled dot-product attention and Softmax — why they’re used\n",
    "1. Dot(Q, K): fast way to compute similarity between query and each key.\n",
    "2. Scaling by sqrt(head_dim): when vectors are high-dimensional, dot products tend to have larger variances. Dividing by sqrt(d_k) keeps the magnitude of the scores moderate so the softmax does not become overly peaky (i.e., prevents numerical saturation and vanishing gradients).\n",
    "3. Softmax over keys: converts arbitrary real-valued scores into positive weights that sum to 1 (a probability distribution). This allows attention to be a convex combination of values:\n",
    "   - Positive, normalized weights -> interpretable attention distribution.\n",
    "   - Smooth, differentiable mapping -> gradients propagate back into Q and K.\n",
    "4. Masking before softmax: setting future positions to -inf ensures softmax gives them (near) zero weight — this enforces causal/ autoregressive behavior.\n",
    "\n",
    "Numerical note: without scaling, for large head_dim, dot products can be so large that softmax's gradients vanish (everything becomes near 0/1). The scaling counters that.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-head attention: what and why\n",
    "- Instead of a single attention, we split the embed dimension into H heads of smaller dimension (head_dim = embed_dim / H).\n",
    "- Each head learns different projection matrices and can specialize (syntax vs semantics, local vs long-range).\n",
    "- The outputs from heads are concatenated (and optionally projected) to yield a richer representation than a single head of the same total size.\n",
    "\n",
    "---\n",
    "\n",
    "## Causal mask and autoregression\n",
    "- For language modeling / next-token prediction we must not allow position t to \"see\" tokens > t.\n",
    "- A triangular (lower) mask sets scores for future keys to -inf before softmax so they get zero attention weight.\n",
    "- This preserves the autoregressive property needed to predict tokens sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical tips for this notebook\n",
    "- To keep notebook outputs/state, add the above Markdown cells via the web editor (do not replace or delete existing code cells).\n",
    "- If you want to experiment further:\n",
    "  - Add residual connections + LayerNorm around attention and feedforward blocks.\n",
    "  - Try weight tying between token embeddings and final projection (W_out = embedding_matrix^T).\n",
    "  - Visualize attn weights for each head (heatmaps) for different inputs.\n",
    "  - Increase dataset size / change inputs to exercise long-range attention.\n",
    "\n",
    "---\n",
    "\n",
    "## Short glossary\n",
    "- embed_dim: full model embedding dimension.\n",
    "- num_heads: number of attention heads.\n",
    "- head_dim: embed_dim / num_heads (dimension per head).\n",
    "- Q, K, V: Query, Key, Value matrices produced from embeddings by learned linear maps.\n",
    "- Softmax: converts scores into normalized weights.\n",
    "- Mask: a boolean matrix that blocks attention to disallowed positions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c180be-1e00-49b0-9f6c-963316033408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b1965-70e9-4089-ae68-dd3d1bd53b1a",
   "metadata": {},
   "source": [
    "Step 1: Prepare the vocabulary and input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9b491b-9be2-4c8d-8765-b37e194a1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['the', 'sun', 'rises', 'in', 'the', 'east']\n",
    "vocab = sorted(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9332acf0-9bc1-499f-bbed-e10c465f74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3cca6e-fe7f-4ab7-b70f-3c0a7e809037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514ec067-460f-4460-bf28-564a122e85a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'east': 0, 'in': 1, 'rises': 2, 'sun': 3, 'the': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0378b45b-fc36-4c3c-8280-600155410555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'east', 1: 'in', 2: 'rises', 3: 'sun', 4: 'the'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869cf46a-6d9e-48cb-8536-9262354492c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [['the', 'sun', 'rises'], ['sun', 'rises', 'in']]\n",
    "targets = ['in', 'the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6075090b-f05f-43b5-b4c8-ab4f2effcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[word2idx[tok] for tok in seq] for seq in inputs])\n",
    "Y = torch.tensor([word2idx[tok] for tok in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd9e3807-8324-4e9c-b529-4ad4ff6193c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 3, 2],\n",
       "        [3, 2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468d3285-b40c-49a1-98c5-9e8f0d6c57f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5bbb0-6a3d-4abe-a5e2-097eb310ee87",
   "metadata": {},
   "source": [
    "Step 2: Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1d7c99-a544-4ea9-97a7-4edb3e04de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "head_dim = embed_dim // num_heads\n",
    "seq_len = X.shape[1]\n",
    "batch_size = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28b11dc7-83eb-46f5-b67a-7f5ef11ff485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c919d364-d955-48fd-9fa5-56ddbba50c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989df238-a6ea-442c-8af4-8db5070c0553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb61bf-d779-40a3-be3c-44fb616bf8b5",
   "metadata": {},
   "source": [
    "Learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7491bd5-1e7f-48bb-ac7a-4cc2bb0742bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = Parameter(torch.randn(vocab_size, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bdad98a-c77e-4304-8a8a-dc15ae896159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.8164,  0.3393, -0.1962,  0.0686,  1.8330, -1.5634, -0.6825,  1.2403],\n",
       "        [-0.4157, -2.8158, -0.4880,  0.9893, -0.2380,  2.2371,  0.6282,  2.2796],\n",
       "        [ 1.2535,  0.3401, -0.0283, -0.7157, -0.8639, -1.7989, -0.3398,  0.6687],\n",
       "        [-0.0123, -0.3947, -0.0860,  0.5334,  2.1976, -0.2144, -0.2776, -0.4011],\n",
       "        [ 0.5143,  1.3045,  1.6402, -0.1149, -0.5363, -0.8710,  1.6064, -0.6619]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e16eb0fb-746a-43f7-ace0-b716081e31af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ddf862b-6945-4cfa-a791-f744786846ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = Parameter(torch.randn(embed_dim, embed_dim))\n",
    "W_k = Parameter(torch.randn(embed_dim, embed_dim))\n",
    "W_v = Parameter(torch.randn(embed_dim, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f743abf-4ea8-44bf-8888-402d9cf80375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0829, -0.1412, -3.1731, -1.4237,  1.5461,  0.6367,  0.0915, -1.0132],\n",
       "        [-0.0592, -1.1897,  0.1291,  2.0549,  1.4506, -0.6190,  0.3577, -0.3925],\n",
       "        [ 0.2888, -1.0396, -0.3008,  0.9865,  2.4813, -1.2502,  0.1523, -0.0751],\n",
       "        [-1.0965,  0.9943, -0.2630,  0.4882,  0.5478,  0.4314,  0.4888,  1.3243],\n",
       "        [ 1.4615,  0.1642, -0.3472, -2.4003,  1.5152,  0.1648,  1.0022, -0.6238],\n",
       "        [ 0.3859,  0.4654, -0.1196,  0.8294,  0.3183,  0.5795, -0.4047,  0.3449],\n",
       "        [ 0.8210, -0.5370, -0.8223,  1.0911,  0.3955, -0.8302, -1.6158,  0.8659],\n",
       "        [ 0.2091,  0.7154,  1.6497,  2.4070,  1.2231,  0.2212,  0.2300, -0.0381]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1484e7d-3fd3-48c5-884c-f06a3eb77eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c22644af-5ed5-473b-8ca3-a170cd7cfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = Parameter(torch.randn(embed_dim, embed_dim))\n",
    "b1 = Parameter(torch.zeros(embed_dim))\n",
    "W2 = Parameter(torch.randn(embed_dim, embed_dim))\n",
    "b2 = Parameter(torch.zeros(embed_dim))\n",
    "W_out = Parameter(torch.randn(embed_dim, vocab_size))\n",
    "b_out = Parameter(torch.zeros(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7bddd1f-d3d8-4bcc-b37a-0ea66a71cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = Parameter(torch.randn(seq_len, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b745314c-eeb8-48f7-a5de-643cad07b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    embedding_matrix, pos_embedding, W_q, W_k, W_v,\n",
    "    W1, b1, W2, b2, W_out, b_out\n",
    "], lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cbd29-5461-4363-948d-932fe0c0047c",
   "metadata": {},
   "source": [
    "Training one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0545a830-0367-4e81-b7b9-4c014047a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa4cebaa-8d87-4154-9cc6-15ab3224d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding + positional encoding\n",
    "embedded = embedding_matrix[X] + pos_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "813dea1b-6501-4a11-8611-d01f2a718310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4510,  1.0351,  1.9332,  2.2591, -1.1639,  0.0343,  1.8987,\n",
       "           0.1107],\n",
       "         [ 0.2814, -2.5231,  0.2681,  1.0828,  1.8583, -0.5441, -0.9493,\n",
       "          -0.2009],\n",
       "         [ 0.4394,  1.7338, -0.9409, -2.2016, -0.7979, -3.4903, -0.5249,\n",
       "           0.4646]],\n",
       "\n",
       "        [[ 0.9244, -0.6640,  0.2071,  2.9074,  1.5701,  0.6909,  0.0147,\n",
       "           0.3715],\n",
       "         [ 1.5472, -1.7883,  0.3258, -0.1663, -1.2033, -2.1286, -1.0114,\n",
       "           0.8689],\n",
       "         [-1.2298, -1.4221, -1.4007, -0.4967, -0.1719,  0.5457,  0.4431,\n",
       "           2.0755]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5f8d8df-e0de-47f0-aee1-9bcb549dc324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ecfd524-e025-433d-a9cb-d15f6e760135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to Q, K, V\n",
    "Q = embedded @ W_q\n",
    "K = embedded @ W_k\n",
    "V = embedded @ W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "edf429ea-8310-48b6-9c19-19ccfb80eec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0ba1a38-39af-4952-acb3-dc331a7a55a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.2061,  -2.3157,  -6.6248,   8.2316,   8.9131,  -2.8827,  -2.3213,\n",
       "            3.3478],\n",
       "         [  0.7008,   4.1779,  -1.7151, -11.2236,   0.0549,   2.6075,   3.2635,\n",
       "           -0.0420],\n",
       "         [ -0.8436,  -4.4766,   1.5845,   0.5003,  -2.3060,  -2.1825,   1.0087,\n",
       "           -5.1491]],\n",
       "\n",
       "        [[ -0.5143,   4.1722,  -3.8729,  -3.3425,   5.6317,   2.7242,   2.6552,\n",
       "            2.4163],\n",
       "         [ -2.9747,   1.3817,  -2.2572,  -3.5271,  -1.3228,   1.2132,   0.9596,\n",
       "           -2.0028],\n",
       "         [  1.0835,   4.3006,   7.3249,   3.5491,  -5.0849,   2.0132,  -1.7091,\n",
       "            1.8517]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a95a63c-9ab7-4d66-8bdc-99867ef34a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "360315f3-c7df-406f-a99e-22d0cf7bbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for multi-head attention\n",
    "def reshape(x):\n",
    "    return x.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26c69961-873d-4d07-80fb-91f28880e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qh, Kh, Vh = map(reshape, (Q, K, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fb5a7a6-8c46-4fc8-926a-79d23f38c868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8604af99-3da1-424e-8245-407b1e563782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -2.2061,  -2.3157,  -6.6248,   8.2316],\n",
       "          [  0.7008,   4.1779,  -1.7151, -11.2236],\n",
       "          [ -0.8436,  -4.4766,   1.5845,   0.5003]],\n",
       "\n",
       "         [[  8.9131,  -2.8827,  -2.3213,   3.3478],\n",
       "          [  0.0549,   2.6075,   3.2635,  -0.0420],\n",
       "          [ -2.3060,  -2.1825,   1.0087,  -5.1491]]],\n",
       "\n",
       "\n",
       "        [[[ -0.5143,   4.1722,  -3.8729,  -3.3425],\n",
       "          [ -2.9747,   1.3817,  -2.2572,  -3.5271],\n",
       "          [  1.0835,   4.3006,   7.3249,   3.5491]],\n",
       "\n",
       "         [[  5.6317,   2.7242,   2.6552,   2.4163],\n",
       "          [ -1.3228,   1.2132,   0.9596,  -2.0028],\n",
       "          [ -5.0849,   2.0132,  -1.7091,   1.8517]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70b286e7-2490-4be1-a128-e0558d559758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal mask to prevent attending to future tokens\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f38a0ac-0581-4924-aed4-b471e0abd814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0.],\n",
       "          [1., 1., 0.],\n",
       "          [1., 1., 1.]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d809b6f-a074-4fe1-a9c7-e16598d1f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scaled dot-product attention\n",
    "scores = (Qh @ Kh.transpose(-2, -1)) / math.sqrt(head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7be89b5e-e682-4e4d-95f4-2b049896e8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 25.2680, -11.2582,   9.3553],\n",
       "          [  9.8371, -10.7304,  10.1255],\n",
       "          [-13.9345,  11.5954, -15.3323]],\n",
       "\n",
       "         [[ -8.7544, -12.1842,  31.9708],\n",
       "          [-14.7860,   2.7654,   7.0745],\n",
       "          [ 17.9632,   7.2769, -15.7331]]],\n",
       "\n",
       "\n",
       "        [[[ -7.3364,   0.1837, -33.9196],\n",
       "          [ -2.7744,  -8.3566, -22.2177],\n",
       "          [ 16.7993,  -9.9005,  21.8704]],\n",
       "\n",
       "         [[-13.9629,   6.5034,  19.9367],\n",
       "          [  4.2783,   2.4653,  -4.0848],\n",
       "          [  6.6419, -13.4783,  -6.3278]]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3258ec0-70c3-4060-af5c-5480ce39fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.masked_fill(mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cd6e863-c73e-4910-8df1-6908c935c69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 25.2680,     -inf,     -inf],\n",
       "          [  9.8371, -10.7304,     -inf],\n",
       "          [-13.9345,  11.5954, -15.3323]],\n",
       "\n",
       "         [[ -8.7544,     -inf,     -inf],\n",
       "          [-14.7860,   2.7654,     -inf],\n",
       "          [ 17.9632,   7.2769, -15.7331]]],\n",
       "\n",
       "\n",
       "        [[[ -7.3364,     -inf,     -inf],\n",
       "          [ -2.7744,  -8.3566,     -inf],\n",
       "          [ 16.7993,  -9.9005,  21.8704]],\n",
       "\n",
       "         [[-13.9629,     -inf,     -inf],\n",
       "          [  4.2783,   2.4653,     -inf],\n",
       "          [  6.6419, -13.4783,  -6.3278]]]], grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02a7c10e-932c-4728-9c80-d6f18b3a957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = F.softmax(scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f3dada2-eb46-4468-b5f1-803bda5cfcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0000e+00, 1.1685e-09, 0.0000e+00],\n",
       "          [8.1755e-12, 1.0000e+00, 2.0204e-12]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3852e-08, 1.0000e+00, 0.0000e+00],\n",
       "          [9.9998e-01, 2.2855e-05, 2.3220e-15]]],\n",
       "\n",
       "\n",
       "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9625e-01, 3.7503e-03, 0.0000e+00],\n",
       "          [6.2359e-03, 1.5824e-14, 9.9376e-01]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.5972e-01, 1.4028e-01, 0.0000e+00],\n",
       "          [1.0000e+00, 1.8276e-09, 2.3298e-06]]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c029794e-adf0-4b39-8e9a-ebe6ed6b0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply attention weights to values\n",
    "attn_output = attn_weights @ Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad9a42fe-edac-4d30-996d-3aebd2e96bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  1.6323,  -7.3257,  -4.4774,   1.5864],\n",
       "          [  1.6323,  -7.3257,  -4.4774,   1.5864],\n",
       "          [ -4.4277,  -0.3395,   3.6692,   3.1080]],\n",
       "\n",
       "         [[ -1.3689,  -1.3359, -10.3603,  -2.7999],\n",
       "          [  0.4137,  -3.0978,   0.9074,   2.0655],\n",
       "          [ -1.3689,  -1.3359, -10.3601,  -2.7998]]],\n",
       "\n",
       "\n",
       "        [[[ -0.8769,  -3.0010,   1.1872,   2.2387],\n",
       "          [ -0.8901,  -3.0026,   1.1822,   2.2474],\n",
       "          [ -1.7953,   2.4614,  -1.1720,   1.0276]],\n",
       "\n",
       "         [[  0.0782,   1.9823,  -4.5615,   0.8238],\n",
       "          [ -0.7249,   1.3781,  -4.1267,   0.4926],\n",
       "          [  0.0782,   1.9823,  -4.5615,   0.8238]]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ac4e055-aa7e-467c-873e-1dc309f224b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate multi-head outputs\n",
    "attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31ca2476-5a8e-4f81-8676-d206d1b439aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  1.6323,  -7.3257,  -4.4774,   1.5864,  -1.3689,  -1.3359, -10.3603,\n",
       "           -2.7999],\n",
       "         [  1.6323,  -7.3257,  -4.4774,   1.5864,   0.4137,  -3.0978,   0.9074,\n",
       "            2.0655],\n",
       "         [ -4.4277,  -0.3395,   3.6692,   3.1080,  -1.3689,  -1.3359, -10.3601,\n",
       "           -2.7998]],\n",
       "\n",
       "        [[ -0.8769,  -3.0010,   1.1872,   2.2387,   0.0782,   1.9823,  -4.5615,\n",
       "            0.8238],\n",
       "         [ -0.8901,  -3.0026,   1.1822,   2.2474,  -0.7249,   1.3781,  -4.1267,\n",
       "            0.4926],\n",
       "         [ -1.7953,   2.4614,  -1.1720,   1.0276,   0.0782,   1.9823,  -4.5615,\n",
       "            0.8238]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d1134e7-772a-4b9f-a2ec-fd25b9ec55ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c672690c-dbc3-451b-a53c-cc917b3f0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward network\n",
    "ffn = torch.relu(attn_output @ W1 + b1)\n",
    "ffn = ffn @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5accc5ab-65d3-4723-87e5-77c4a134b010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.5664,  13.5499,  27.6570,  63.4399, -43.8924,  -0.5320, -20.1907,\n",
       "           35.3387],\n",
       "         [  6.3338,  12.6149,  15.1894,  25.7221, -19.6118,  -7.8365,  -9.7928,\n",
       "            8.9380],\n",
       "         [ -1.2811, -14.2705,  11.0819,  23.9507, -33.0082,   4.6001,  -9.2054,\n",
       "           14.9693]],\n",
       "\n",
       "        [[  1.9444,  -7.8102,  13.5497,  24.8362, -32.8808,  -5.3498, -13.8637,\n",
       "           10.4085],\n",
       "         [  4.3597,  -2.8524,  13.8946,  24.0713, -28.3426,  -6.1110,  -9.9676,\n",
       "           11.2889],\n",
       "         [-11.5424, -23.9073,  -3.3235,  -0.7025, -18.3985,   9.8123, -11.0779,\n",
       "            0.4239]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "979688a1-1ca4-4a50-ad5d-f2f1fdc25cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfe5f654-6236-4d12-8d8e-124eb17462aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the output for the last token position\n",
    "final_token = ffn[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cee5fa32-e801-4923-8c8d-d8e41eccff81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.2811, -14.2705,  11.0819,  23.9507, -33.0082,   4.6001,  -9.2054,\n",
       "          14.9693],\n",
       "        [-11.5424, -23.9073,  -3.3235,  -0.7025, -18.3985,   9.8123, -11.0779,\n",
       "           0.4239]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "550eeebf-fee6-4693-8bb0-128df69a0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 51.3055, -82.7576, -60.5026, -60.0071,  15.2986],\n",
      "        [  9.0535, -58.4483,   1.8867, -34.0361,  19.8173]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor(67.0316, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Output projection to vocab\n",
    "logits = final_token @ W_out + b_out\n",
    "print(logits)\n",
    "\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af0792f5-643e-46cf-8afe-e2c237d7c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_indices = torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b0ca635-806d-4ee8-86e0-00f76c4520ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_words = [idx2word[i.item()] for i in predicted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6347868b-3aa6-462d-a911-be72fd91fb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['east', 'the']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a2b4aac-80db-4e9e-81ee-9c5aabdad1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query = job seeker’s resume (what they want).\n",
    "#Keys = job postings (what’s available).\n",
    "#Values = job descriptions (actual info you’d get)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb7a4c-1363-495f-bcf1-26175e5c598b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python gen_ai",
   "language": "python",
   "name": "gen_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
