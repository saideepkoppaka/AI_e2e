# AI_e2e: End-to-End AI Fundamentals

This repository features a series of Jupyter notebooks designed to help you master Python, data science, statistics, machine learning, deep learning, and NLP, with a special focus on building models from scratch.

---

## ðŸš€ Getting Started: Running the Notebooks

### 1. Clone the Repository

```bash
git clone https://github.com/saideepkoppaka/AI_e2e.git
cd AI_e2e
```

### 2. Create and Activate a Virtual Environment

#### On **Linux/MacOS**:
```bash
python3 -m venv venv
source venv/bin/activate
```

#### On **Windows**:
```bash
python -m venv venv
venv\Scripts\activate
```

### 3. Install Requirements

```bash
pip install -r requirements.txt
```

### 4. Launch Jupyter Lab

```bash
jupyter lab
```

Your browser will open Jupyter Lab. Open any `.ipynb` file and run the cells!

---

## ðŸ“š Notebook Summaries

### 1. `python_start.ipynb`
**Summary:**  
A gentle introduction to Python for AI. Covers:
- Why Python is essential for AI/ML
- Basic syntax, print statements
- Imports, standard library usage
- Platform-agnostic features

---

### 2. `ML_Statistics_Notebook.ipynb`
**Summary:**  
Core statistics for machine learning, including:
- Mean, median, mode, variance, standard deviation
- Probability basics
- Normal distribution

---

### 3. `pandas_fundamentals_with_visuals.ipynb`
**Summary:**  
Hands-on pandas with a sample employee dataset:
- DataFrame creation and manipulation
- Visualizations with matplotlib
- Advanced pandas operations (reshaping, rolling windows)

---

### 4. `Logistic_Regression_Tutorial.ipynb`
**Summary:**  
A practical walkthrough of logistic regression using scikit-learn:
- Data loading and preprocessing
- Model training and evaluation
- Hyperparameter tuning and metrics (accuracy, ROC, confusion matrix)
- Visualization with seaborn/matplotlib

---

### 5. `PyTorch_DeepLearning_Intro.ipynb`
**Summary:**  
Step-by-step deep learning in PyTorch:
- Neural network from scratch
- Tensors, DataLoader, model layers, activation
- Training loop, loss, optimizer
- Regularization & evaluation using the Iris dataset

---

### 6. `Next_Word_Prediction_Evolution.ipynb`
**Summary:**  
Evolution of next-word prediction in NLP:
- Basic Python approaches
- Regex-based patterns
- N-gram language model
- Stepwise code explanations and results

---

### 7. `RNN_deepdive.ipynb`
**Summary:**  
A hands-on exploration of RNNs:
- Data preparation for NLP
- Custom RNN implementation using PyTorch
- Sequence modeling for next-word prediction

---

### 8. `Transformer_mini.ipynb`
**Summary:**  
Building a minimal transformer model:
- Explanation of attention mechanism
- Creating vocab, encoding, and model logic from scratch
- Stepwise implementation and experiment

---

### 9. `Transformer_final.ipynb`
**Summary:**  
Full transformer implementation:
- Decoder-only transformer with causal masking
- Data prep with <sos>/<eos> tokens
- Attention mask, inference, and advanced logic

---

> **Note:** More notebooks may exist. [Browse all notebooks here.](https://github.com/saideepkoppaka/AI_e2e/search?q=extension%3Aipynb&type=code)

---

## âœ¨ Author

Maintained by [saideepkoppaka](https://github.com/saideepkoppaka)
