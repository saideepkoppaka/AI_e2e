{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863db149",
   "metadata": {},
   "source": [
    "# NLP Evolution for Sentiment Analysis - From Rules to Transformers\n",
    "\n",
    "This notebook demonstrates how sentiment analysis has evolved in the field of Natural Language Processing (NLP), moving through different eras:\n",
    "\n",
    "1. Simple Python using keyword matching\n",
    "2. Regex-based pattern matching\n",
    "3. Naive Bayes from scratch using token frequencies\n",
    "4. RNN (Recurrent Neural Network) using PyTorch\n",
    "5. Transformer model using PyTorch\n",
    "\n",
    "All models are trained from scratch with minimal datasets to serve as a clear educational guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d21391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Sentiment Analysis Using Simple Python and Keywords\n",
    "# -------------------------------------------------------------\n",
    "# This is the simplest form of sentiment analysis using fixed word lists.\n",
    "# It checks for presence of positive or negative words in the sentence.\n",
    "import re\n",
    "\n",
    "def simple_sentiment_analysis(text):\n",
    "    positive_words = ['good', 'happy', 'joy', 'awesome', 'excellent']\n",
    "    negative_words = ['bad', 'sad', 'pain', 'terrible', 'awful']\n",
    "    \n",
    "    text = text.lower()\n",
    "    pos_count = sum(1 for word in positive_words if word in text)\n",
    "    neg_count = sum(1 for word in negative_words if word in text)\n",
    "\n",
    "    if pos_count > neg_count:\n",
    "        return 'Positive'\n",
    "    elif neg_count > pos_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "print(\"Simple Python:\", simple_sentiment_analysis(\"I feel good and happy today!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Using Regex-Based Features\n",
    "# -------------------------------------\n",
    "# Regular expressions help match word boundaries more accurately.\n",
    "\n",
    "def regex_sentiment_analysis(text):\n",
    "    pos_pattern = r\"\\b(good|happy|joy|awesome|excellent)\\b\"\n",
    "    neg_pattern = r\"\\b(bad|sad|pain|terrible|awful)\\b\"\n",
    "    \n",
    "    pos_matches = re.findall(pos_pattern, text, flags=re.IGNORECASE)\n",
    "    neg_matches = re.findall(neg_pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    if len(pos_matches) > len(neg_matches):\n",
    "        return 'Positive'\n",
    "    elif len(neg_matches) > len(pos_matches):\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "print(\"Regex-based:\", regex_sentiment_analysis(\"This is a terrible and awful situation.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Naive Bayes From Scratch (Basic NLP)\n",
    "# ----------------------------------------------\n",
    "# Introduces tokenization, vocabulary, and probability-based classification.\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "class NaiveBayesSentiment:\n",
    "    def __init__(self):\n",
    "        self.word_probs = defaultdict(lambda: {'positive': 1, 'negative': 1})\n",
    "        self.class_probs = {'positive': 1, 'negative': 1}\n",
    "        self.vocab = set()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    def train(self, texts, labels):\n",
    "        for text, label in zip(texts, labels):\n",
    "            self.class_probs[label] += 1\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                self.word_probs[token][label] += 1\n",
    "                self.vocab.add(token)\n",
    "\n",
    "    def predict(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        total = sum(self.class_probs.values())\n",
    "        log_probs = {}\n",
    "        for label in self.class_probs:\n",
    "            log_prob = math.log(self.class_probs[label] / total)\n",
    "            for token in tokens:\n",
    "                if token in self.vocab:\n",
    "                    word_count = self.word_probs[token][label]\n",
    "                    total_words = sum(self.word_probs[t][label] for t in self.vocab)\n",
    "                    log_prob += math.log(word_count / total_words)\n",
    "            log_probs[label] = log_prob\n",
    "        return max(log_probs, key=log_probs.get)\n",
    "\n",
    "texts = [\"I love this movie\", \"This movie is terrible\", \"Happy and joyful experience\", \"Bad and sad ending\"]\n",
    "labels = ['positive', 'negative', 'positive', 'negative']\n",
    "\n",
    "nb_model = NaiveBayesSentiment()\n",
    "nb_model.train(texts, labels)\n",
    "print(\"Naive Bayes:\", nb_model.predict(\"What a joyful and lovely experience\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd96505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: RNN for Sentiment Analysis (from scratch using PyTorch)\n",
    "# --------------------------------------------------------------------\n",
    "# We use an RNN model for sentiment classification trained on a small dataset.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Basic Dataset\n",
    "rnn_texts = [\"I love this movie\", \"This movie is terrible\", \"Happy and joyful experience\", \"Bad and sad ending\"]\n",
    "rnn_labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Tokenization and Vocabulary\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "vocab = set(word for sentence in rnn_texts for word in tokenize(sentence))\n",
    "word2idx = {word: idx+1 for idx, word in enumerate(vocab)}\n",
    "word2idx['<PAD>'] = 0\n",
    "\n",
    "def encode(text):\n",
    "    return [word2idx[word] for word in tokenize(text)]\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = [torch.tensor(encode(text)) for text in texts]\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    lengths = [len(t) for t in texts]\n",
    "    padded = nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    return padded, torch.tensor(labels)\n",
    "\n",
    "dataset = SentimentDataset(rnn_texts, rnn_labels)\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=10, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.rnn(x)\n",
    "        return torch.sigmoid(self.fc(h_n.squeeze(0)))\n",
    "\n",
    "model = RNNModel(len(word2idx))\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(10):\n",
    "    for inputs, labels in loader:\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Test Prediction\n",
    "def predict_sentiment_rnn(text):\n",
    "    model.eval()\n",
    "    encoded = torch.tensor(encode(text)).unsqueeze(0)\n",
    "    output = model(encoded)\n",
    "    return 'Positive' if output.item() > 0.5 else 'Negative'\n",
    "\n",
    "print(\"RNN:\", predict_sentiment_rnn(\"What a joyful experience\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a093049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Transformer for Sentiment Analysis (PyTorch)\n",
    "# --------------------------------------------------------\n",
    "# Implementing a small transformer for sentiment classification\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=16, nhead=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)  # Transformer expects [seq_len, batch, embed_dim]\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "transformer_model = TransformerModel(len(word2idx))\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train Transformer Model\n",
    "for epoch in range(10):\n",
    "    for inputs, labels in loader:\n",
    "        outputs = transformer_model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Prediction with Transformer\n",
    "def predict_sentiment_transformer(text):\n",
    "    transformer_model.eval()\n",
    "    encoded = torch.tensor(encode(text)).unsqueeze(0)\n",
    "    output = transformer_model(encoded)\n",
    "    return 'Positive' if output.item() > 0.5 else 'Negative'\n",
    "\n",
    "print(\"Transformer:\", predict_sentiment_transformer(\"An awesome and joyful time!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python gen_ai",
   "language": "python",
   "name": "gen_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
